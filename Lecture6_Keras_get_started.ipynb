{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import scipy as sc\n",
    "import math as ma\n",
    "from scipy import linalg, optimize, constants, interpolate, special, stats\n",
    "from math import exp, pow, sqrt, log\n",
    "\n",
    "import seaborn as sns #spezielle Graphikdarstellungen\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "#den oberen Teil kennt man schon vom letzten Mal...\n",
    "#Theano kommt neu dazu - \n",
    "#Library für effiziente Berechnungen mit großen Matrizen -> DeepLearning!\n",
    "import theano \n",
    "import tensorflow\n",
    "\n",
    " \n",
    "#Jetzt noch scikit-learn:\n",
    "#hier stecken viele Funktionalitäten drin, die man gut brauchen kann: \n",
    "#Fehlerfunktionen, Standard-Modelltypen, Preprocessing-Algorithmen, Daten... \n",
    "\n",
    "import sklearn as sl \n",
    "from sklearn import model_selection, metrics, datasets\n",
    "from sklearn.preprocessing import OneHotEncoder \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir richtig loslegen lesen wir nochmal die MNIST-Daten ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testdaten einlesen\n",
    "\n",
    "data_file_test = open('./mnist_test.csv', 'r')\n",
    "data_list_test = data_file_test.readlines() \n",
    "data_file_test.close()\n",
    "\n",
    "#Trainingsdaten einlesen\n",
    "data_file_train = open('./mnist_train.csv', 'r') \n",
    "data_list_train = data_file_train.readlines()\n",
    "data_file_train.close()\n",
    "\n",
    "#Check der Dimensionen \n",
    "print(len(data_list_test)) #10000 Testdatensätze \n",
    "print(data_list_train)\n",
    "print(len(data_list_train)) #60000 Trainingsdatensätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alle Trainingsbeispiele durchgehen - erst leere Liste erzeugen\n",
    "inputs_train = np.empty([60000,784])\n",
    "targets_train = np.empty([60000,10])\n",
    " \n",
    "\n",
    "k = 0\n",
    "for record in data_list_train:    \n",
    "    #an Kommas auftrennen\n",
    "    all_values_train = record.split(',')\n",
    "    \n",
    "    #Skalieren und shiften\n",
    "    inputs_t = (np.asfarray(all_values_train[1:]) / 255.0 * 0.999) + 0.0001\n",
    "    \n",
    "    #Target generieren\n",
    "    numberoutputs = 10\n",
    "    targets_t = np.zeros(numberoutputs) + 0.0001\n",
    "    targets_t[int(all_values_train[0])] = 0.999\n",
    "\n",
    "    inputs_train[k]  = inputs_t\n",
    "    targets_train[k] = targets_t\n",
    "    \n",
    "    k += 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Alle Testbeispiele durchgehen - erst leere Liste erzeugen\n",
    "inputs_test = np.empty([10000,784])\n",
    "targets_test = np.empty([10000,10])\n",
    "\n",
    "k = 0\n",
    "for record in data_list_test:    \n",
    "    #an Kommas auftrennen\n",
    "    all_values_test = record.split(',')\n",
    "    #Skalieren und shiften\n",
    "    inputs_te = (np.asfarray(all_values_test[1:]) / 255.0 * 0.999) + 0.0001\n",
    "    #Target generieren\n",
    "    numberoutputs_test = 10\n",
    "    targets_te = np.zeros(numberoutputs_test) + 0.0001\n",
    "    targets_te[int(all_values_test[0])] = 0.999\n",
    "  \n",
    "    inputs_test[k]  = inputs_te\n",
    "    targets_test[k] = targets_te\n",
    "    \n",
    "    k += 1\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#jetzt Daten im Python Format sichern \n",
    "#laden ist später jederzeit mit dem Befehl \"load\" wieder möglich\n",
    "    \n",
    "from tempfile import TemporaryFile\n",
    "mnist_targets_train = TemporaryFile()\n",
    "mnist_inputs_train = TemporaryFile()\n",
    " \n",
    "mnist_targets_test = TemporaryFile()\n",
    "mnist_inputs_test = TemporaryFile()\n",
    " \n",
    "np.save(mnist_targets_train, targets_train)\n",
    "np.save(mnist_inputs_train, inputs_train)\n",
    "np.save(mnist_targets_test, targets_test)\n",
    "np.save(mnist_inputs_test, inputs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Modelle in Keras mit dem Sequential() Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras erlaubt es mit dem \"Sequential\" Modell und dem \"add\" Befehl ein Netz aus verschiedenen Schichten sequentiell aufzubauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() # Modellname ist hier \"model\", kann aber beliebig gewählt werden\n",
    "model.add(Dense(32, input_dim=784) )\n",
    "#input_dim gibt die Dimension des Eingangsvektors an und muß spezifiziert werden\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor man ein Modell trainieren kann muß man den Lernprozeß konfigurieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für ein multi-class Klassifikationsproblem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# # Für ein binäres Klassifikationsproblem\n",
    "# model.compile(optimizer='rmsprop',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Für ein mean squared error Regressionsproblem\n",
    "# model.compile(optimizer='rmsprop',\n",
    "#               loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphische Darstellung der Modelltopologie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training des Modelles:\n",
    "    \n",
    "Keras Modelle werden auf Numpy arrays mit input Daten und labels trainiert.  \n",
    "Um ein Modell zu trainieren benutzt man die \"fit\" Funktion.\n",
    "Doku: https://keras.io/models/sequential/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-input Modell mit 2 Klassen (binary classification):\n",
    "\n",
    "\n",
    "# Dummy Daten\n",
    "\n",
    "# Training\n",
    "x_train = np.random.random((1000, 20)) #Input-Vektor mit Dim = 20\n",
    "y_train = np.random.randint(2, size=(1000, 1)) #Targets: 2 Klassen\n",
    "\n",
    "# Test\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "# Definiere die Topologie des Netzes\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Lernprozeß konfigurieren\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Modell trainieren\n",
    "model1.fit(x_train, y_train,\n",
    "          epochs=20, #Anzahl Trainingsepochen\n",
    "          batch_size=128, #Batch-Size\n",
    "          verbose=0) #print Ausgabe des Trainingsverlaufes 0=nein 1=ja\n",
    "\n",
    "# Performance auswerten\n",
    "score = model1.evaluate(x_test, y_test, batch_size=128) #Auswertung des Modelles auf den Testdaten\n",
    "print(score)\n",
    "print(model1.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model1, to_file='model1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für ein MLP Modell mit 10 Klassen (categorical classification):\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD #Stochastic Gradient Descent\n",
    "\n",
    "# Dummy Daten\n",
    "\n",
    "# Training\n",
    "x_train = np.random.random((1000, 20)) # Input-Vektor mit Dim = 20\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10) # Target mit 10 Klassen\n",
    "\n",
    "# Test\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "# Definiere die Topologie des Netzes\n",
    "model2 = Sequential()\n",
    "# Dense(64) ist ein fully-connected Layer mit 64 hidden units.\n",
    "# in der ersten Schicht muß der input data shape festgelegt werden:\n",
    "# hier 20-dim. Vektor\n",
    "\n",
    "model2.add(Dense(64, activation='relu', input_dim=20))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) #Parameter beim SGD Lernrate, Momentum, ...\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          verbose=0)\n",
    "score = model2.evaluate(x_test, y_test, batch_size=128)\n",
    "print(score)\n",
    "print(model2.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model2, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute eines Modelles\n",
    "\n",
    "- model.layers: zeigt die verwendeten schichten im Modell an\n",
    "- model.input und model.output: zeigen welche Inputs/Outputs das Modell generiert\n",
    "- model.summary(): gibt eine Zusammenfassung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model1.layers)\n",
    "print(model1.input)\n",
    "#print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Erstes Beispiel in Keras: 3-Layer MLP zur Klassifikation der MNIST-Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Daten sind schon vorbereitet (Transformation der Daten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs_train.shape)\n",
    "print(targets_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere die Topologie des Netzes\n",
    "modelMNIST = Sequential()\n",
    "# Dense(64) ist ein fully-connected Layer mit 64 hidden units.\n",
    "# in der ersten Schicht muß der input data shape festgelegt werden:\n",
    "# hier 20-dim. Vektor\n",
    "\n",
    "modelMNIST.add(Dense(20, activation='sigmoid', input_dim=784))\n",
    "modelMNIST.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) #Parameter beim SGD Lernrate, Momentum, ...\n",
    "modelMNIST.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy']) #das ist die \"Hit-Rate\" = Anteil der richtig klassifizierten Beispiele\n",
    "\n",
    "historyMNIST=modelMNIST.fit(inputs_train, targets_train,\n",
    "          epochs=10,\n",
    "          batch_size=200,\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = modelMNIST.evaluate(inputs_test, targets_test, batch_size=1) #erster Wert loss, zweiter Wert accuracy hier 95%\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(modelMNIST, to_file='modelMNIST.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Visualisierung des Lernens - zeichne Fehler während des Trainings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dazu eine fertige Funktion mit entsprechenden Plot-Routinen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updatable plot\n",
    "# a minimal example (sort of)\n",
    "from IPython.display import clear_output\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...ein Beispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere die Topologie des Netzes\n",
    "modelMNIST = Sequential()\n",
    "# Dense(64) ist ein fully-connected Layer mit 64 hidden units.\n",
    "# in der ersten Schicht muß der input data shape festgelegt werden:\n",
    "# hier 20-dim. Vektor\n",
    "\n",
    "modelMNIST.add(Dense(20, activation='sigmoid', input_dim=784))\n",
    "modelMNIST.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) #Parameter beim SGD Lernrate, Momentum, ...\n",
    "modelMNIST.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy']) #das ist die \"Hit-Rate\" = Anteil der richtig klassifizierten Beispiele\n",
    "\n",
    "historyMNIST=modelMNIST.fit(inputs_train, targets_train,\n",
    "          epochs=20,\n",
    "          batch_size=100,\n",
    "          verbose=0,\n",
    "          validation_data=(inputs_test, targets_test),                  \n",
    "          callbacks=[plot_losses])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geht auch mit Hilfe der history - allerdings erst, wenn das Training abgeschlossen ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotte Training & Validation accuracy (Hit Rate)\n",
    "plt.plot(historyMNIST.history['accuracy'])\n",
    "plt.plot(historyMNIST.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plotte Training & Validation loss (hängt von der Loss-Funktion beim Training ab)\n",
    "plt.plot(historyMNIST.history['loss'])\n",
    "plt.plot(historyMNIST.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Ein Beispiel für ein Regressionsproblem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wir nehmen wieder ein gängiges Benschmark-Beispiel (z.B. von Kaggle)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston Housing Daten:\n",
    "\n",
    "The dataset describes 13 numerical properties of houses in Boston suburbs and is concerned with modeling the price of houses in those suburbs in thousands of dollars. As such, this is a regression predictive modeling problem. Input attributes include things like crime rate, proportion of nonretail business acres, chemical concentrations and more.\n",
    "\n",
    "This is a well-studied problem in machine learning. It is convenient to work with because all of the input and output attributes are numerical and there are 506 instances to work with.\n",
    "\n",
    "\n",
    "\n",
    "Housing Values in Suburbs of Boston\n",
    "\n",
    "The medv variable is the target variable.\n",
    "\n",
    "##  Data description\n",
    "\n",
    "The Boston data frame has 506 rows and 14 columns.\n",
    "\n",
    "This data frame contains the following columns:\n",
    "\n",
    "crim\n",
    "per capita crime rate by town.\n",
    "\n",
    "zn\n",
    "proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "indus\n",
    "proportion of non-retail business acres per town.\n",
    "\n",
    "chas\n",
    "Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "\n",
    "nox\n",
    "nitrogen oxides concentration (parts per 10 million).\n",
    "\n",
    "rm\n",
    "average number of rooms per dwelling.\n",
    "\n",
    "age\n",
    "proportion of owner-occupied units built prior to 1940.\n",
    "\n",
    "dis\n",
    "weighted mean of distances to five Boston employment centres.\n",
    "\n",
    "rad\n",
    "index of accessibility to radial highways.\n",
    "\n",
    "tax\n",
    "full-value property-tax rate per $10,000.\n",
    "\n",
    "ptratio\n",
    "pupil-teacher ratio by town.\n",
    "\n",
    "black\n",
    "1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "\n",
    "lstat\n",
    "lower status of the population (percent).\n",
    "\n",
    "medv\n",
    "median value of owner-occupied homes in $1000s.\n",
    "\n",
    "Source\n",
    "\n",
    "Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81–102.\n",
    "\n",
    "Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load dataset\n",
    "dataframe = pd.read_csv(\"../Daten/housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values # Keras erwartet Numpy-array keinen Pandas Dataframe!\n",
    "# split in input (X) und output (Y) Variablen (letzte Spalte enthält das Target!)\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in Training und Validierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain_unscaled=X[0:400]\n",
    "YTrain_unscaled=Y[0:400]\n",
    "XTest_unscaled=X[400:]\n",
    "YTest_unscaled=Y[400:]\n",
    "XTest_unscaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(dataframe);\n",
    "#plt.savefig(\"PairPlotHousing.pdf\", dpi=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten auf Mittelwert 0 und Varianz 1 skalieren: StandardScaler von ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scalerX = preprocessing.StandardScaler().fit(XTrain_unscaled)\n",
    "X_train_scaled= scalerX.transform(XTrain_unscaled)\n",
    "print(np.mean(X_train_scaled))\n",
    "\n",
    "scalerXtest = preprocessing.StandardScaler().fit(XTest_unscaled)\n",
    "X_test_scaled= scalerXtest.transform(XTest_unscaled)\n",
    "print(np.mean(X_test_scaled))\n",
    "\n",
    "scalerY = preprocessing.StandardScaler().fit(YTrain_unscaled.reshape(-1,1))\n",
    "Y_train_scaled= scalerY.transform(YTrain_unscaled.reshape(-1,1))\n",
    "print(np.mean(Y_train_scaled))\n",
    "\n",
    "scalerYtest = preprocessing.StandardScaler().fit(YTest_unscaled.reshape(-1,1))\n",
    "Y_test_scaled= scalerYtest.transform(YTest_unscaled.reshape(-1,1))\n",
    "print(np.mean(Y_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3-layer MLP für Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHousing = Sequential()\n",
    "modelHousing.add(Dense(50, input_dim=13, kernel_initializer='normal', activation='tanh'))\n",
    "modelHousing.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
    "# Compile model\n",
    "sgd = SGD(lr=0.1,  momentum=0.9, nesterov=True) #Parameter beim SGD: Lernrate, Momentum, ..\n",
    "modelHousing.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "\n",
    "historyHousing=modelHousing.fit(X_train_scaled, Y_train_scaled,\n",
    "          shuffle=True,\n",
    "          epochs=50,\n",
    "          batch_size=100,\n",
    "          verbose=0,\n",
    "          validation_data=(X_test_scaled, Y_test_scaled),                  \n",
    "          callbacks=[plot_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreHousing = modelHousing.evaluate(X_test_scaled, Y_test_scaled, batch_size=1) #erster Wert loss, zweiter Wert accuracy hier 95%\n",
    "scoreHousing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_scaled=modelHousing.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Größe und Auflösung der zusammengesetzten Gesamtgraphik\n",
    "plt.figure(figsize=(6, 6), dpi=90)\n",
    "plt.plot(y_predicted_scaled,Y_test_scaled,'o')\n",
    "plt.axis([-2,2,-2,2]); #Achsenausschnitt\n",
    "plt.xlabel('y-Predicted (scaled)') #Beschriftung x-Achse\n",
    "plt.ylabel('y-obeserved (scaled)') #Beschriftung y-Achse \n",
    "plt.title('Scatter Plot: Model vs. Obs'); #Titel der Graphik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_unscaled=scalerYtest.inverse_transform(y_predicted_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Größe und Auflösung der zusammengesetzten Gesamtgraphik\n",
    "plt.figure(figsize=(6, 6), dpi=90)\n",
    "plt.plot(y_predicted_unscaled,YTest_unscaled,'o')\n",
    "#plt.axis([-2,2,-2,2]); #Achsenausschnitt\n",
    "plt.xlabel('y-Predicted (scaled)') #Beschriftung x-Achse\n",
    "plt.ylabel('y-obeserved (scaled)') #Beschriftung y-Achse \n",
    "plt.title('Scatter Plot: Model vs. Obs'); #Titel der Graphik\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1)\n",
    "\n",
    "- Experimentieren Sie mit den verschiedenen Meta-Parametern im MNIST-Netz (Aktivierungsfunktion, fehlerfunktion, Lernrate, Epochen, Anzahl Hidden-Neuronen,...)\n",
    "- bauen Sie weitere Schichten in das Netz ein und testen Sie die Performance\n",
    "- Testen Sie verschiedene Regularisierungsverfahren (Dropout, WeightDecay,...) und fassen Sie das Ergebnis kurz zusammen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2)\n",
    "\n",
    "- Experimentieren Sie mit den verschiedenen Meta-Parametern im Housing-Netz (Lernrate, Epochen, Anzahl Hidden-Neuronen,...)\n",
    "- macht es Sinn eine Sigmoid-Aktivierung oder ReLU zu benutzen?\n",
    "- bauen Sie weitere Schichten in das Netz ein und testen Sie die Performance\n",
    "- wäre hier EarlyStopping sinnvoll?\n",
    "- versuchen Sie herauszufinden welche Inputs für das Modell besonders sinnvoll sind (dazu muß man evtl. an den Daten selber etwas tun)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
