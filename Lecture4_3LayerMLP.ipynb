{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sl\n",
    "%matplotlib inline\n",
    "import scipy as sc\n",
    "import math as ma\n",
    "from scipy import linalg, optimize, constants, interpolate, special, stats\n",
    "from math import exp, pow, sqrt, log\n",
    "\n",
    "import seaborn as sns #spezielle Graphikdarstellungen\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ziel: eigenes 3-layer MLP auf Basis der gelernten Konzepte!\n",
    "\n",
    "Read: Tariq Rashid: \"Make your own Neural Network\"\n",
    "\n",
    "\n",
    "Jetzt werden die einzelnen Bestandteile eines 3-Layer MLPs zusammengesetzt...  \n",
    "\n",
    "Die Klasse \"MLP\" sollte mindestens drei Eigenschaften haben:  \n",
    "\n",
    "1) Initialisierung der Gewichte mit Zufallszahlen  \n",
    "2) Training = Gewichtanpassung anhand von Trainingsbeispielen  \n",
    "3) In-Out Abfrage: wird ein Input ins trainierte Netz eingespeist soll eine Ausgabe erfolgen    \n",
    "\n",
    "Also sind die Grundelemente der Klasse wie folgt zu definieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class definition für Neural Network object\n",
    "class neuralNetwork:\n",
    "\n",
    "    #Initialization \n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    #Training \n",
    "    def train():\n",
    "        pass \n",
    "\n",
    "    #Query\n",
    "    def query():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst soll die Initialisierung durchgeführt werden.  \n",
    "Die Anzahl der Gewichte in den Matrizen hängt von der Dimension des Inputs und der Anzahl der Neuronen in der Hidden und Output-Schicht ab:  \n",
    "\n",
    "       -> freie Parameter = Meta-Parameter  \n",
    "       \n",
    "Die Lernrate ist ein weiterer wichtiger Parameter, der berücksichtigt werden muß."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class definition für Neural Network object \n",
    "class neuralNetwork:\n",
    "\n",
    "    #Initialisierung\n",
    "\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "\n",
    "        #setze Nummer der nodes in jeder input, hidden & output layer\n",
    "    \n",
    "        self.inodes = inputnodes \n",
    "        #self sorgt dafür, daß die Funktion beim Erzeugen dem richtigen Objekt zugewiesen wir\n",
    "        self.hnodes = hiddennodes \n",
    "        self.onodes = outputnodes\n",
    "        #learning rate \n",
    "        self.lr = learningrate \n",
    "        pass\n",
    "    \n",
    "    #Training \n",
    "    def train():\n",
    "        pass\n",
    "    \n",
    "    #Query\n",
    "    def query():\n",
    "        pass\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jetzt müssen die entsprechenden Gewichtsmatrizen zwischen Input und Hidden (Größe hiddennodes x inputnodes) und Hidden und Output (Größe outputnodes x hiddennodes) angelegt und für den Anfang mit zufälligen kleinen Gewichten belegt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class definition für Neural Network object\n",
    "class neuralNetwork:\n",
    "    #Initialisierung\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "                 \n",
    "        #setze Nummer der nodes in jeder input, hidden & output layer\n",
    "\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes \n",
    "        self.onodes = outputnodes\n",
    "        #Gewichtmatrizen input->hidden wih und hidden->output who\n",
    "        #die Gewichte innerhalb der arrays sind w_i_j,\n",
    "        #womit das Neuron i mit dem j-ten der nächsten Schicht verbunden wird\n",
    "        self.wih = np.random.normal(0.0, pow(self.hnodes,-0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes,-0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "    \n",
    "        #learning rate \n",
    "        self.lr = learningrate \n",
    "        pass\n",
    "    \n",
    "    #Training \n",
    "    def train():\n",
    "        pass\n",
    "    #Query\n",
    "    def query():\n",
    "        pass\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...das Einzige was für die Initialisierung jetzt noch fehlt sind die Definitionen der Aktivierungsfunktionen, die in der Hidden und Output-Schicht verwendet werden sollen...\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class definition für Neural Network object \n",
    "\n",
    "class neuralNetwork:\n",
    "    \n",
    "    #Initialisierung\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        #setze Nummer der nodes in jeder input, hidden & output layer\n",
    "        self.inodes = inputnodes #self sorgt dafür, daß die Funktion beim Erzeugen dem richtigen Objekt zugewiesen wird                     \n",
    "        self.hnodes = hiddennodes \n",
    "        self.onodes = outputnodes\n",
    "    \n",
    "        #Gewichtmatrizen input->hidden wih und hidden->output who\n",
    "        #die Gewichte innerhalb der arrays sind w_i_j,\n",
    "        #womit das Neuron i mit dem j-ten der nächsten Schicht verbunden wird\n",
    "    \n",
    "        self.wih = np.random.normal(0.0, pow(self.hnodes,-0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes,-0.5), (self.onodes, self.hnodes))\n",
    "    \n",
    "    #learning rate \n",
    "        self.lr = learningrate \n",
    "        pass\n",
    "    \n",
    "    #Aktivierungsfunktion der Hidden Schicht z.B. Sigmoid (logistic activation) Funktion\n",
    "    #gebräuchlich wäre auch tanh oder ReLU\n",
    "    #hier wird diese Aktivierung auch in der Ausgabeschicht verwendet\n",
    "\n",
    "        self.activation_function = lambda x: special.expit(x) #lambda sorgt dafür, Fkt. anonym bleibt\n",
    "        pass\n",
    "\n",
    "    #Aktivierungsfunktion der Output Schicht - hängt stark vom Problem und der Loss-Funktion ab\n",
    "    #1) Regression: square Loss + Identity\n",
    "    #2) Klassifikation: cross entropy loss + sigmoid/softmax\n",
    "    \n",
    "    #Training \n",
    "    def train():\n",
    "        pass\n",
    "    \n",
    "    #Query\n",
    "    def query():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die bisherigen Schritte testen zu können ist es geschickt erstmal an der Query-Funktion weiterzuarbeiten.\n",
    "Dann könnte man einen Vorwärtspfad durch das initialisierte Netz durchlaufen und eine Ausgabe generieren.\n",
    "Der Query-Funktion müssen nur die Inputs übergeben werden...\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class definition für Neural Network object \n",
    "\n",
    "class neuralNetwork:\n",
    "#Initialisierung\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        #setze Nummer der nodes in jeder input, hidden & output layer\n",
    "        self.inodes = inputnodes \n",
    "        self.hnodes = hiddennodes \n",
    "        self.onodes = outputnodes\n",
    "    \n",
    "        #Gewichtmatrizen input->hidden wih und hidden->output who\n",
    "        #die Gewichte innerhalb der arrays sind w_i_j,\n",
    "        #womit das Neuron i mit dem j-ten der nächsten Schicht verbunden wird\n",
    "        self.wih = np.random.normal(0.0, pow(self.hnodes,-0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes,-0.5), (self.onodes, self.hnodes))\n",
    "    \n",
    "        #learning rate \n",
    "        self.lr = learningrate \n",
    "        pass\n",
    "    \n",
    "        #Aktivierungsfunktion der Hidden Schicht z.B. Sigmoid (logistic activation) Funktion\n",
    "        #gebräuchlich wäre auch tanh oder ReLU\n",
    "        #hier wird diese Aktivierung auch in der Ausgabeschicht verwendet\n",
    "        self.activation_function = lambda x: special.expit(x) #lambda sorgt dafür, Fkt. anonym bleibt\n",
    "        pass\n",
    "\n",
    "        #Aktivierungsfunktion der Output Schicht - hängt stark vom Problem und der Loss-Funktion ab\n",
    "    \n",
    "        #1) Regression: square Loss + Identity\n",
    "        #2) Klassifikation: cross entropy loss\n",
    "        \n",
    "    #Training \n",
    "    def train():\n",
    "        pass\n",
    "\n",
    "    #Query\n",
    "    def query(self, inputs_list):\n",
    "   \n",
    "        #sicherstellen, daß die Liste der Inputs ein 2D NumPy array ist \n",
    "        #für die anschl. Rechnungen muß das array noch transponiert werden \n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        #Berechnung der Eingangssignale in die Hiddenschicht \n",
    "        #d.h. Multiplikation der ersten Matrix mit dem Input-array \n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        \n",
    "        #Berechnung der Ausgangssignale der Hiddenschicht\n",
    "        #d.h. Anwendung der Aktivierungsfunktion\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        #Berechnung der Eingangssignale in die Output-Schicht \n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        \n",
    "        #Berechnung der Ausgangssignale der Output-Schicht \n",
    "        #d.h. Anwendung der Aktivierungsfunktion \n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs \n",
    "        pass\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jetzt mal ein allererster Test:  \n",
    "1) Generieren eines speziellen Neuro-Objektes aus der oben definierten Klasse  \n",
    "2) Einsetzen von einem zufälligen Vektor mit geeigneter Dimension\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Festlegen der Neuro-Struktur durch Definition der Neuronenzahlen\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "\n",
    "#Lernrate \n",
    "learning_rate = 0.3\n",
    "#Erstellen des konkreten Neuro-Objektes\n",
    "n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jetzt kann man einen Vorwärtspfad berechnen und das Ergebnis ausgeben lassen..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77490519],\n",
       "       [0.30535185],\n",
       "       [0.63949353]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.query([1.2, 2.0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letzter fehlender Schritt:  \n",
    "\n",
    "## Training des Netzes!  \n",
    "\n",
    "Für das Training sind eigentlich nur zwei Dinge relevant:  \n",
    "die Inputs und natürlich die Targets - an denen wird der Fehler berechnet und hinterher für die Backpropagation genutzt\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class definition für Neural Network object\n",
    "class neuralNetwork:\n",
    "    #Initialisierung\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        #setze Nummer der nodes in jeder input, hidden & output layer\n",
    "        self.inodes = inputnodes #self sorgt dafür, daß die Funktion beim Erzeugen dem richtigen Objekt zugewiesen wird\n",
    "        self.hnodes = hiddennodes \n",
    "        self.onodes = outputnodes\n",
    "\n",
    "        #Gewichtmatrizen input->hidden wih und hidden->output who\n",
    "        #die Gewichte innerhalb der arrays sind w_i_j,\n",
    "        #womit das Neuron i mit dem j-ten der nächsten Schicht verbunden wird\n",
    "        \n",
    "        self.wih = np.random.normal(0.0, pow(self.hnodes,-0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes,-0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        #learning rate \n",
    "        self.lr = learningrate \n",
    "        pass\n",
    "    \n",
    "        #Aktivierungsfunktion der Hidden Schicht z.B. Sigmoid (logistic activation) Funktion\n",
    "        #gebräuchlich wäre auch tanh oder ReLU\n",
    "        #hier wird diese Aktivierung auch in der Ausgabeschicht verwendet\n",
    "        \n",
    "        self.activation_function = lambda x: special.expit(x) #lambda sorgt dafür, Fkt. anonym bleibt\n",
    "        pass\n",
    "    \n",
    "        #Aktivierungsfunktion der Output Schicht - hängt stark vom Problem und der Loss-Funktion ab\n",
    "        #1) Regression: square Loss + Identity\n",
    "        #2) Klassifikation: cross entropy loss + sigmoid/softmax\n",
    "        \n",
    "    #Training\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        \n",
    "        #Inputs und Targets als 2D NumPy array schreiben \n",
    "        inputs = np.array(inputs_list, ndmin=2).T \n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        #Eingangssignale in die Hidden Schicht\n",
    "        hidden_inputs = np.dot(self.wih, inputs) #Ausgangssignale aus der Hiddenschicht\n",
    "        hidden_outputs = self.activation_function(hidden_inputs) #Eingangssignale in Ausgabeschicht\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    " \n",
    "        #Berechnung der Ausgangssignale der Output-Schicht\n",
    "        #d.h. Anwendung der Aktivierungsfunktion\n",
    "        final_outputs = self.activation_function_out(final_inputs)\n",
    "        \n",
    "        #Berechnung des Fehlers Output-Target oder Target-Output - keine spezifische Fehlerfunktion\n",
    "        output_errors = final_outputs - targets\n",
    "        \n",
    "        #Berechnung der lokalen Fehler im Backpropagation Fehlerfluß: \n",
    "        errors_hid = weights.T_hid_out*errors_out\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        \n",
    "        #Update der Gewichte von Hidden nach Output auf Basis der berechneten Fehler\n",
    "        self.who += self.lr*np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "       \n",
    "        # += heißt einfach nur zähle zur Variable die neue Menge dazu: x +=3 -> x = x+3\n",
    "        \n",
    "        #Update der Gewichte von Input nach Hidden auf Basis der berechneten Fehler\n",
    "        self.wih += self.lr*np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "        pass\n",
    "    \n",
    "    #Query\n",
    "    def query(self, inputs_list):\n",
    "        \n",
    "        #sicherstellen, daß die Liste der Inputs ein 2D NumPy array ist \n",
    "        #für die anschl. Rechnungen muß das array noch transponiert werden \n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        #Berechnung der Eingangssignale in die Hiddenschicht \n",
    "        #d.h. Multiplikation der ersten Matrix mit dem Input-array \n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        \n",
    "        #Berechnung der Ausgangssignale der Hiddenschicht\n",
    "        #d.h. Anwendung der Aktivierungsfunktion\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        #Berechnung der Eingangssignale in die Output-Schicht \n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "                  \n",
    "        #Berechnung der Ausgangssignale der Output-Schicht\n",
    "        #d.h. Anwendung der Aktivierungsfunktion \n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erster Check der Performance an einem standardmäßigen Klassifikationsbeispiel:  \n",
    "    handgeschriebene Zahlen erkennen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "#Testdaten einlesen\n",
    "\n",
    "data_file_test = open('../Daten/MNIST/mnist_test.csv', 'r')\n",
    "data_list_test = data_file_test.readlines() \n",
    "data_file_test.close()\n",
    "\n",
    "#Trainingsdaten einlesen\n",
    "data_file_train = open('../Daten/MNIST/mnist_train.csv', 'r') \n",
    "data_list_train = data_file_train.readlines()\n",
    "data_file_train.close()\n",
    "\n",
    "#Check der Dimensionen \n",
    "print(len(data_list_test)) #10000 Testdatensätze \n",
    "print(len(data_list_train)) #60000 Trainingsdatensätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list_train;\n",
    "data_list_test[0]; \n",
    "#Datenstruktur: jedes Element ist ein langer String \n",
    "#mit den kommaseparierten Intensitäten und der Klasse vorne "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "die Struktur der Daten ist wie folgt:\n",
    "     \n",
    "     \n",
    "'7 an der ersten Stelle ist das Label - es handelt sich also um ein Bild der handgeschriebenen Zahl 7  \n",
    "\n",
    "die folgenden 28x28 = 784 Zahlen sind die kodierten Farbwerte der einzelnen Pixel des Bildes (Wertebereich: 0-255)  \n",
    "\n",
    "Jetzt könnte man sich erstmal so ein Bild ansehen.  \n",
    "Dazu muß das Format der einzelnen Pattern geändert werden:  \n",
    "\n",
    "1) den langen kommaseparierten Datensatz in Einzelwerte splitten  \n",
    "2) den ersten Wert als Target behalten  \n",
    "3) Rest der 28x28 Datensätze in ein 28x28 array umformen  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_values_first_example = data_list_test[0].split(',') #Daten aufteilen\n",
    "image_array = np.asfarray(all_values_first_example[1:]).reshape((28,28)) #28x28 array draus machen\n",
    "plt.imshow(image_array, cmap='Greys', interpolation = 'None'); #plotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_values_first_example[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt erstmal eine kleine Testmenge daraus 100 Trainingsbeispiele und 10 Testdatensätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_list_train_small = data_list_train[0:100] \n",
    "data_list_test_small = data_list_test[0:10]\n",
    "                                 \n",
    "len(data_list_train_small[0])\n",
    "\n",
    "all_values_first_example1 = data_list_test_small[9].split(',') #Daten aufteilen \n",
    "image_array1 = np.asfarray(all_values_first_example1[1:]).reshape((28,28)) #28x28 array draus machen\n",
    "plt.imshow(image_array1, cmap='Greys', interpolation = 'None'); #plotten\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Wertebereiche sind für Neuro nicht gut geeignet also: skalieren der Intensitätswerte auf [0,1]  \n",
    "Beispiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_example = (np.asfarray(all_values_first_example1[1:])/255.0 * 0.99) + 0.01\n",
    "scaled_example;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt noch das Target generieren:  \n",
    "Sinnvoll ist es hier als Zielfunktion eine Liste zu generieren.  \n",
    "Es gibt Zahlen von 0 - 9, also 10 verschiedene Zielklassen. \n",
    "\n",
    "Als Target-Vektor wird standardmäßig ein 10-dim Vektor gewählt,  \n",
    "der an allen Stellen 0 hat außer an der Stelle der zutreffenden Klasse.  \n",
    "Dort ist der Wert dann 1  \n",
    " -> Stichwort: One-Hot – Codierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.99]\n"
     ]
    }
   ],
   "source": [
    "#10 verschiedene Klassen \n",
    "onodes = 10\n",
    "\n",
    "#Vektor der Länge 10 mit lauter Nullen -\n",
    "# 0.01 sorgt nur dafür, daß keine Probleme mit z.B. teilen durch Null entstehen\n",
    "targets = np.zeros(onodes) + 0.01\n",
    "\n",
    "#jetzt wird der 0-te Eintrag gelesen = Targetklasse\n",
    "#dann wird eine ganze Zahl draus gemacht\n",
    "#dann wird im Null-Vektor an dieser Stelle eine 1 (bzw. 0.99 eingesetzt)\n",
    "targets[int(all_values_first_example1[0])] = 0.99 \n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Jetzt kann man im Prinzip das erste Beipiel mit dem simplen MLP bearbeiten..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## das machen wir dann nächstes Mal - die Spannung steigt! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
